Q-Learning with Function Approximation 

Reasoning:
Q-learning with function approximation is suitable for Sphere Wars because our environment has a very large state space, consisting of 162 tiles, ownership, resources, and piece locations for each player. A tabular Q-table would be too large and would not generalize across similar states. By using a small neural network to approximate the Q-function, the agent can generalize across similar board configurations and learn patterns (e.g. expansion towards unclaimed tiles or spawning when it has enough resources). This approach preserves the simplicity and stability of standard Q-learning while enabling the agent to operate effectively in a large, combinatorial environment. 

Implementation would consist of a neural network receiving a compact representation of the game state as input and outputs estimated Q-values for each possible action, which are trained using temporal-difference (TD) learning.

Reward System:
We will keep the reward structure intentionally simple, directly tying the rewards to the core objective of expanding territory. The agent receives +1 reward every time it successfully captures a new tile, and 0 reward for other actions, including moving, spawning, and revisiting tiles it already owns. This sparse reward system ensures that the learning signal reflects only the meaningful strategic progress and avoids unintended incentives such as unnecessary movement or resource hoarding. The simplicity of this reward system also makes it easier to analyze the learned behaviours of the agent and compare different learning methods later in the project.

Simple Pseudocode of Training Loop:
1. Initialize small neural network that takes the current game state as input and outputs Q-values for all possible actions
2. At the start of training, begin each episode by resetting the environment to its initial state
3. For each step of the episode:
    - Use an epsilon-greedy policy to choose an action
        - With probability epsilon, choose a random action (exploration)
        - Otherwise, choose the action with the highest predicted Q-value (exploitation)
    - Apply the action in the environment and observe the resulting next state, the reward and whether the episode has ended
    - Compute the target Q-value 
        - If the episode continues, the target is the reward + discounted max Q-value of the next state
        - Otherwise, the target is just the reward
    - Compute the loss as the squared difference between the network's current Q-value estimate for the chosen action and the target value
    - Update the gradient on the network to reduce the loss
    - Advance the current state to the next state
4. After each episode, decay epsilon so that the agent gradually shifts from exploration to exploitation
5. Repeat for many episodes until the policy converges or performance stabilizes